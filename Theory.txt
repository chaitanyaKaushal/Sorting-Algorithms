There are so many things in our real life that we need to search for, like a particular record in database, roll numbers in merit list, a particular telephone number in 
telephone directory, a particular page in a book etc. All this would have been a mess if the data was kept unordered and unsorted, but fortunately the concept of sorting
came into existence, making it easier for everyone to arrange data in an order, hence making it easier to search.

Now, that we understand the need of sorting , we can now focus on its efficiency.

The efficiency of a sorting algorithm can be explained on the basis of
1) Time Complexity -> Most critical for real-time applications and when our application has to serve large number of customers.
2) Space Complexity -> the extra hard-drive memory required to execute our program.
3) Memory Complexity-> here , the memory refers to RAM memory.
P.S. When the dataset is huge , then memory becomes a critical priority for any industry because we cannot always buy hard-drive/external storage devices to serve our needs keeping in mind
     their costs.

My project focuses on the sorting techniques and when to apply which algorithm to make it most efficient.

I have used the data set of Missouri sales-tax-rates.[https://catalog.data.gov/dataset/sales-tax-rates]
Let's study the efficiency of our algorithms in this dataset.

n is the input size.

1)Bubble Sort:
	It follows swapping between two adjacent elements.This way, in the first pass(walkthrough/traversal),the maximum element is positioned at the last index of the array.
	Now, we continue our comparisons-swaps procedure for the next array.length()-1 elements.So , for an ith pass the ith largest element gets placed at (array.length() - i)th position.
	
	1)worst case time-complexity:when array is in descending order. O(n^2)
	      It works best when our array is almost sorted. Then complexity is O(n). This is because our flag variable would become false once our array gets sorted in (say) ith iteration
	      Then we break out of our loops.
	2) space-complexity: O(1) constant space because we haven't used any extra array for the sorting procedure.
	3) memory-complexity: O(n) : memory occupied by our original array.

2)Insertion Sort:
	We use a key element which assumes that all elements lying before the key's index are sorted. So, now it is our turn to sort the key element and find its appropiate position in the
	already sorted portion.(sorted portion is [0,key.index -1]).
	Repeat the procedure for i in (1,n-1)
	
	1)worst-case time complexity: same as bubble sort. O(n^2) when the array is in descending order.
	      Works best for nearly sorted arrays with achievable complexity of O(n) because here the number of movements(positioning the key) are reduced to some constant value i.e. O(1).
	2)space-complexity: O(1) constant space because we haven't used any extra array for the sorting procedure.
	3)memory-complexity : O(n) :memory occupied by our original array.

	Note: Insertion sort is the only comparison-based sorting technique which is online i.e it can sort even if new elements are getting appended to the end of the array while sorting
	      is going on.

3)Selection Sort: It finds out the minimum element at each iteration of the existing active region of our array and place swap it's actual index position with the first index of the active
		  region of our array.
	Note-> Time Complexity is indifferent to any arrangement of array elements. It is O(n^2).
	Space Complexity: O(1)
	Memory Complexity O(n)
	
	Note-> It is unstable algorithm and cannot maintain the actual order of duplicate elements unlike insertion,bubble and merge sorts.

4)Quick Sort: This algorithm uses the concept of a pivotal element. All elements smaller than or equal to the pivotal element are shifted towards the left of the pivot and the ones greater
	      than pivot are placed towards right.
	      This procedure is repeated recursively on the recursive stack till out start index does not become greater than/equal to end index.
	      The algorithm is highly dependent on choice of pivotal element.
	      Usually, we take pivotal element to be either the first,last or middle most element. Again , we cannot say what our time complexity would be since, we have many many choices 	     	      for pivotal positions. Generally , middle element is safe to take as pivotal.
	
	      1) Time Complexity:
			a)Best and Average Case : O(n*logn)
			b)Worst Case : O(n^2): (say)pivotal element is taken to be the first index. Then
				b.1) Array is already sorted in same order.
				b.2) Array is already sorted in reverse order.
				b.3) All elements are same (special case of case 1 and 2)
				
	      2) Space Complexity: O(1)
	      3) Memory Complexity: O(n*logn):when we call partition function for every recursive stack.
	
	Note->When array is nearly sorted, either bubble or insertion sort can be used because of O(n) complexity against O(n^2) for quick sort.

5)Merge Sort: This is also a recursive algorithm like quick sort where we follow divide and conquer strategy. We split our current array into two equal halves by calculating
	      mid=(start+end)>>1 and making two halves [start,mid],[mid+1,end]. Then we apply merge sort on these two arrays.Then, this procedure is repeated for those two sub parts.
	      After obtaining two sorted halves , we finally merge those two sub parts into one single array which is sorted using merge() function.
	
	      1)Time Complexity: O(n*logn) for any arrangement of the elements. 
	      2)Space Complexity: O(n): we use an extra array of same size as that of our original array in our merge() function.
	      3)Memory Complexity: O(n)

	Because of merge sort's indifferent behaviour, it is widely accepted algorithm for sorting.
	
	NOTE-> When input size and arrangement of data elements are not known , merge Sort is always the best choice.


